---
title: "Relation between Age and Household Income"
author: "Anthony Yasan, Preston O'Connor, Khoa Dao, Matthew Jacob, Nick Wierzbowski"
date: "2025-02-26"
output: pdf_document
editor_options: 
  markdown: 
    wrap: sentence
---

We are modeling the linear regression of the Dependent Income, Independent Age in our model

# Introduction

Our model is a simple linear regression representing the relationship between the feature age and result household income in the United States.
For our data we chose to use an extract from IPUMS USA, a database that provides access to census and survey data for research purposes.
For our variable selection, we chose individual age and household income; although this selection may seem counter intuitive, we thought it pertinent to avoid bias in contributions to household income stemming from homemaking and dependents.
In this data extract household income includes every member’s individual annual incomes added up at time of the census, referring to either their current annual salary or their earnings in the last 12 months.
Age is simply their current age at the time of the census, and to avoid issues we filtered our sample to include only 18-65 year old individuals to capture working age incomes.
The significance of this relation is its connection to the causes of income inequality in the United States.
If income is closely tied to age, this suggests that income inequality is dependent on age bracket and correlated years of experience, providing evidence of a degree of social mobility across class.
If this were the case, it may dissipate political will from the lower classes to demand redistributive policies, according to the POUM (prospect of upward mobility) hypothesis advanced by Benanbou and Ok.
However, a simple linear regression is definitely not the best tool we could use to analyze this relation, as explained by Solon that due to poor data selection (not analyzing the same households or individuals across many years), and his more advanced model produces a higher correlation between income and intergenerational income (which is a slightly different question that ours).
The packages used were ipumsr, dplyr, ggplot2 and caTools.
Ipumsr provides an easy interface to access IPUMS data in r and even create extracts.
Dplyr provides many useful data manipulation tools, which include such functions as mutate and filter.
Ggplot2 provides the tools to easily create graphics representing data and statistics.
CaTools provides utility functions for moving window statistics.
Our results primarily indicate the weakness of the model in relating our two variables, showing close to no correlation between different individuals ages and their incomes.
Again, considering the more ideal elements of a similar analysis described by Solon this is not a terribly surprising outcome.

## Installing the R-packages

```{r, include = FALSE }
# remove comments out these blocks to install the R packages that are being used
install.packages("ipumsr") # for the data set
install.packages("dplyr") # for the data set
install.packages("caTools") # use this for the set seed of the training set
install.packages("ggplot2") # graph and display data

# Code to implement the R packages
library(ipumsr) 
library(dplyr)
library(ggplot2) 
library(caTools) 
```

# Data description

The dataset is sourced from IPUMS USA, which provides microdata extracted from the U.S.
Census and American Community Survey (ACS).
It contains demographic and economic data at both household and individual levels.

The dataset consists of 3,405,809 rows and 15 columns.

However, for model implementation we randomly selected a seed of 200,000 rows to utilize for efficiency and for the modeling portion of our data.

Each observation represents a household.

Here are the key variables included in the dataset:\
- YEAR: Census year (e.g., 2023).\
- SAMPLE: IPUMS sample identifier.\
- SERIAL: Unique household serial number.\
- CBSERIAL: Original Census Bureau household serial number.\
- HHWT: Household weight for proper representation.\
- CLUSTER: Household cluster for variance estimation.\
- STRATA: Household strata for variance estimation.\
- GQ: Group quarters status (e.g., household, institution).\
- HHINCOME: Total household income for all members over 15 years old.
- PERNUM: Person number within the household.\
- PERWT: Person weight for population estimates.\
- SEX: Gender classification (Male/Female).\
- AGE: Individual’s age in years.\
- RACE: General race classification.\
- RACED: Detailed race classification.

## Information about the Data set

```{r}
file_path <- "student_study_hours.csv"
data <- read.csv(file_path, header = TRUE, stringsAsFactors = FALSE)
X_data <- data$Hours
Y_data <- data$Scores
summary(data)
```

## Table of Data

```{r}
#View(data)
dim(data)

```

## Data Cleaning and Outlier Removal

```{r}
# select the age and the Total Household income as the main columns of interest, then filter based of 18 <= age <= 65 (Thats the only age bracket that is currently working and has other factors effecting income). Also, there is a negative value in household income which doesn't make sense at all, so we filter it out. 

# ask if the filter crashes out after a certain amount on the computer and if we need to shrink the train size
 data <- data %>%
   select(Hours, Scores) %>%
   mutate(Scores = as.numeric(Scores), Hours = as.numeric(Hours))
#   filter(!is.na(HHINCOME), !is.na(AGE)) %>%
#   filter(between(AGE, 18, 65)) %>%
#   filter(HHINCOME > 0)

# dim(data)  #if you want to view the two filtered columns

IQR_of_AGE <- IQR(X_data, na.rm = TRUE)
IQR_of_HHINCOME <- IQR(Y_data, na.rm = TRUE)


AGE_lower <- quantile(X_data, 0.25, na.rm = TRUE) - 1.5 * IQR_of_AGE
AGE_upper <- quantile(X_data, 0.75, na.rm = TRUE) + 1.5 * IQR_of_AGE

HHINCOME_lower <- quantile(Y_data, 0.25, na.rm = TRUE) - 1.5 * IQR_of_HHINCOME
HHINCOME_upper <- quantile(Y_data, 0.75, na.rm = TRUE) + 1.5 * IQR_of_HHINCOME

#continue to filter any of the outliers
filtered_data <- data %>%
  filter(Hours >= AGE_lower & Hours <= AGE_upper)%>%
    filter(Scores >= HHINCOME_lower & Scores <= HHINCOME_upper)

# we check and see the data sets we have filtered out of here
dim(filtered_data) 

X_filtered_data <- filtered_data$Hours
Y_filtered_data <- filtered_data$Scores

```

## Original Histogram

### Age

We can see in the histogram for Age that there is a relatively normal distribution with no extreme skew or any outliers present at all.

```{r}
ggplot(data, aes(x = Hours)) +
  geom_histogram(binwidth = 5, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Age", x = "Age", y = "Frequency") +
  theme_minimal()
```

### Household Income

We can see here that pre-filtering our data has a very large right skew and an extremely large outlier present in the data set.This is due to the fact that most individuals earn a relatively modest and moderate incomes.

It's likely this data set interviewed a lot of households who come from this category.

```{r}
ggplot(data, aes(x = Scores)) +
  geom_histogram(binwidth = 10000, fill = "green", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Household Income", x = "Household Income", y = "Frequency") +
  theme_minimal()
```

## Filtered Histogram

### AGE

We can see in the histogram for Age that there is a relatively normal distribution with no skew and major outliers.
so we have a fairly balanced data set.

```{r}
ggplot(filtered_data, aes(x = Hours)) +
  geom_histogram(binwidth = 5, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Age", x = "Age", y = "Frequency") +
  theme_minimal()
```

### Household Income

Although the data set still contains a right skew, The data is a lot better of a fit for this instance.
There are no extreme outliers and actual as mentioned before that skew is bound to be prevalent over the individual as most house holds in the data set earn a relatively modest income

```{r}
ggplot(filtered_data, aes(x = Scores)) +
  geom_histogram(binwidth = 10000, fill = "green", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Household Income", x = "Household Income", y = "Frequency") +
  theme_minimal()
```

## Box Plots of original data

### AGE

```{R}
boxplot(X_data, main = "Boxplot of Age", col = "lightblue", ylab = "Age")
```

### Household Income

```{R}
boxplot(Y_data, main = "Boxplot of Household Income", col = "lightgreen", ylab = "Household Income")
```

# Analysis

```{r}
# modifying data into a training set and a testing set
set.seed(1)
# ask about a good metric for the split of the data
split <- sample.split(Y_filtered_data, SplitRatio = 0.8)
train_set <- subset(filtered_data, split == TRUE)
test_set <- subset(filtered_data, split == FALSE)

#sized of the sets
cat(paste("Size of the training set (rows):", nrow(train_set), "\n"))
cat(paste("Size of the testing set (rows):", nrow(test_set), "\n"))

#model from the training data
linear_model <- lm(Scores ~ Hours, data = train_set)

# Predicted values on the test set
test_set$Scores <- predict(linear_model, newdata = test_set)

# calculate residuals for the test set
test_set$residuals <- test_set$Scores - test_set$Scores


# Display equation information for the simple linear regression
linear_model
```

For the data set, we implemented 98% (103,723 points) of filtered data options goes towards training our model.
The large percentage was used due to the Extreme scale of data we are implementing.
The Testing set implemented 2% (1,660 points) of the data and we implemented for the testing and assessment of our regression algorithm.

Thus Through our training data we get the The following regression model $$
HHINCOME = 115580.53 + 33.32 \times AGE
$$ Which we can streamline down into the following linear regression problem

$$ 
y = 115580.53 + 33.32 x_1
$$

## Implementing the Plots

### Linear Regression with Training Data

```{r}
plot(train_set$Hours, train_set$Scores, main="HHINCOME vs AGE", xlab="Age", ylab="Household Income")

# Fit the linear regression model
model <- lm(Scores ~ Hours, data = train_set)

# Add the regression line to the plot
abline(model, col="red")

```

We can see with the diaganol with the original training set is that it is almost impossible to deter how good of a fit the line is.
There are simply to many points in the model to decipher a strong linear regression.

### Linear Regression with Testing Data

This plot is also referenced in the next session for further assessment.

```{r}

plot(test_set$Hours, test_set$Scores, main="HHINCOME vs AGE", xlab="Age", ylab="Household Income")


# Add the regression line to the plot
abline(model, col="red")

```

From our model we can see that the linear regression line is an extremely poor fit and in fact actually resembles the set up of the residual vs fitted value graphs.

This output further suggest that a linear regression model is not appropriate for the data, as the residuals show the lower cluster patterns, and the separate data points for the testing points appear to fail to encompass the model with any sort of trends.

### Summary of the Simple Linear Regression Model

```{r}
summary(linear_model)
```

From the Residual ranges we can see that the linear model has some very large error when it comes to underestimating and overestimating a US citizens house hold income.
Our coefficient for the Age of a person is 33.32 meaning that the model predicts that for every additional year of age, household income increases by 33.32 dollars on average.
this is an extremely small change which can only mean the age alone is not a very strong depiction of income in our linear model.

With this Our predicted linear equation is

# Model Evaluation and Prediction

### Model Assessment

```{r}
predicted_HHI <- predict(model, newdata = test_set)
ggplot(test_set, aes(x = predicted_HHI, y = residuals)) +
  geom_point(alpha = 0.5, color = 'black') +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals vs. Fitted Values",
       x = "Fitted Values",
       y = "Residuals") +
  theme_minimal()
```

Observe this Residuals vs. Fitted Values plot.

The first thing we need to consider is whether the residuals are randomly distributed around the zero line.
Here there are clearly more points below the line than above.
This means that our model is likely to overestimate household income and isn't doing a good job.

The second consideration is the pattern of the residuals.
The fact that the residuals appear with in a higher density towards the bottom points towards our model not being a good fit.

The third consideration is the spread of the residuals across the range of the fitted values.
This appears to be good since the graph is horizontally consistent.

The final consideration is if there are any outliers.
We already filtered the data for outliers so this is not a problem here.

```{r}
ggplot(test_set, aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line() +
  labs(title = "Normal Q-Q Plot of Residuals") +
  theme_minimal()
```

The Q-Q plot shown here is meant to help us see how normal our data is.
If the points fall close to the diagonal line, it is normally distributed.
If the data falls far from the line, it is not normally distributed.
In this case, the plot shows significant deviations from the line, especially in the tails, indicating that the residuals are not normally distributed.
This non-normality suggests potential issues with the model, such as missing important predictor variables, the presence of outliers, or non-linearity in the data.
Consequently, the assumption of normally distributed residuals is violated, which can impact the reliability of certain statistical tests and confidence intervals.

**Goodness-of-Fit:** - **R-squared Value:** Indicates the proportion of variance in the dependent variable explained by the model.
- **Adjusted R-squared Value:** Adjusted for the number of predictors in the model.

```{r}
adjusted_r_squared <- summary(linear_model)$r.squared
adjusted_r_squared

```

From our extremely low R-sqaured Value, we can conclude that AGE in this instance has no Variability in household income.
In other words, Age and house hold income's relationship is extremely week

```{r}
adjusted_r_squared <- summary(linear_model)$adj.r.squared
adjusted_r_squared

```

While using the adjusted R-squared model to prevent overfitting of the data, we can see that the data is relatively close to our initial R-sqaured.
Due to the fact the simple linear regression model uses only one independent variable, this would lead to a simaler range in output.this reinforces our interpretation of the age variable not being a significant influence on the household income.

### Model Accuracy

**Error Metrics:** - **Mean Squared Error (MSE):** Measures the average squared difference between actual and predicted values.

```{r}
r <- as.vector(test_set$residuals)
mse <- mean(r^2)
mse


```

The Mean Squared Error displays an extremely high number.this means there is a large difference between the predicted output of a actual and predicted output of someone's income.The output further enforces the inadequacy of the linear regression model.

### Prediction

**Prediction vs. Actual Plot:**

```{r}
plot(test_set$Hours, test_set$Scores, main="Prediction vs Actual", 
     xlab="Age", ylab="Household Income", pch=16, col="blue")

# add linear regression model
abline(model, col="red", lwd=3)



```

# Conclusion and Summary

Our simple linear regression model for the Comparison of United States Household income and resident age from the IPUM USA data.
WE were able to derive a linear regression model from a training set of 103,723 points to derive the Equation below.
$$
HHINCOME = 115580.53 + 33.32 \times AGE
$$ This equation indicates that the increase in a person's age in the discrete model had an almost negligible effect on the amount they could contribute to their household income.
This suggests that, overall, there is minimal income growth on a yearly basis for individuals.
Our R-squared value of 3.51e-05 and adjusted R-squared value of 2.54e-05 show that this model had almost no correlation at all with the testing set.
This was also reflected in the graph of the derived regression equation and the actual predicted values, which followed no discernible trends and resembled a residual plot more than an actual regression model.
Our mean squared error, which resulted in approximately 4.89 billion, highlights the presence of extreme outliers and significant differences between our predicted and actual values.
Through our residual fitted values we can further visualize that the regression model very rarely ever comes close to predicting the actual value of a lot of our points.
When we start to map out the Q-Q plot we also notice that there is a week linear correlation on the diagonal and that the data set is has alot of outliers present on the lower and the upper portions of our model with these heavier tails.

A positive outcome from this model is that it establishes a baseline comparison for future improvements in the modeling process.
This model allows us to explore different mathematical modeling approaches and refine our data processing methods for a more optimal setup.
Additionally, the residual-based calculations emphasized the need to reconsider our original data selection, suggesting that household income may have a much stronger correlation with other independent variables present in the data set.

The model also has drawbacks.
The large data set made it difficult to perform an efficient analysis without long computing times, requiring us to reduce the data size and introduce a seed.
Although this was done through a randomized selection approach, it may have affected the selection of appropriate data.
Furthermore, the data set exhibited potential bias, as a significant portion of those surveyed came from modest to lower-income households, leading to a skewed data set.

Regardless of how we interpret our findings, it is evident that moving to another modeling approach is necessary to better understand the factors affecting household income in the United States.
Given the limitations of simple linear regression, a multivariate linear regression model would be a more optimal approach, allowing us to incorporate additional variables to enable predictive accuracy.
Additionally, having access to more dynamic and optimal testing methods, including Cross Validation selection, F-tests, Residual Error Independence and many more testing methods , will enable us to build a more robust and efficient model.
By refining our techniques, we can develop a stronger predictive model that better represents the complexities of household income distribution.

# Reference

Sources cited:

#1 Steven Ruggles, Sarah Flood, Matthew Sobek, Daniel Backman, Annie Chen, Grace Cooper, Stephanie Richards, Renae Rodgers, and Megan Schouweiler.
IPUMS USA: Version 15.0 [dataset] Minneapolis, MN: IPUMS, 2024.
<https://doi.org/10.18128/D010.V15.0>

#2 Benabou, Roland, and Efe A. Ok. “Social Mobility and the Demand for Redistribution: The Poum Hypothesis.” The Quarterly Journal of Economics 116, no. 2 (2001): 447–87.
<http://www.jstor.org/stable/2696470>.

#3 Solon, Gary.
“Intergenerational Income Mobility in the United States.” The American Economic Review 82, no. 3 (1992): 393–408.
<http://www.jstor.org/stable/2117312>.
