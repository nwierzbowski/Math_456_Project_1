---
title: "Project_1_456"
author: "Anthony Yasan, Preston O'Connor, Khoa Dao"
date: "2025-02-20"
output: pdf_document
editor_options: 
  markdown: 
    wrap: sentence
---

We are modeling the linear regression of the Dependent Income, Independent Age in our model

# Introduction

## Installing the R-packages

```{r, include = FALSE }
# remove comments out these blocks to install the R packages that are being used
#install.packages("ipumsr") # for the data set
#install.packages("dplyr") # for the data set
#install.packages("caTools") # use this for the set seed of the training set
#install.packages("ggplot2") 

# Code to implement the R packages
library(ipumsr) # implemented to get our data set
library(dplyr)
library(ggplot2) # Visiual display of the Graphs and modes implemented
library(caTools) # make specifics seeds and data sets for our overall test and our testing with 
```

# Data description

The dataset is sourced from IPUMS USA, which provides microdata extracted from the U.S.
Census and American Community Survey (ACS).
It contains demographic and economic data at both household and individual levels.

The dataset consists of 3,405,809 rows and 15 columns.

However, for model implementation we randomly selected a seed of 200,000 rows to utilize for the modeling portion of our data.

Each observation represents a household.

Here are the key variables included in the dataset:\
- YEAR: Census year (e.g., 2023).\
- SAMPLE: IPUMS sample identifier.\
- SERIAL: Unique household serial number.\
- CBSERIAL: Original Census Bureau household serial number.\
- HHWT: Household weight for proper representation.\
- CLUSTER: Household cluster for variance estimation.\
- STRATA: Household strata for variance estimation.\
- GQ: Group quarters status (e.g., household, institution).\
- HHINCOME: Total household income for all members over 15 years old.
- PERNUM: Person number within the household.\
- PERWT: Person weight for population estimates.\
- SEX: Gender classification (Male/Female).\
- AGE: Individualâ€™s age in years.\
- RACE: General race classification.\
- RACED: Detailed race classification.

## Information about the Data set

```{r}
ddi <- read_ipums_ddi("usa_00001.xml")
data <- read_ipums_micro(ddi)
dim(data)
summary(data)
```

## Table of Data

```{r}
#View(data)
#The Code struggles to run the data set with 2 million points is two extensive to run
set.seed(11)

s <- sample(1:nrow(data), size = 200000)
data <- data[s, ]
dim(data)

```

## Data Cleaning and Outlier Removal

```{r}
# select the age and the Total Household income as the main columns of interest, then filter based of 18 <= age <= 65 (Thats the only age bracket that is currently working and has other factors effecting income). Also, there is a negative value in household income which doesn't make sense at all, so we filter it out. 

# ask if the filter crashes out after a certain amount on the computer and if we need to shrink the train size
data <- data %>%
  select(AGE, HHINCOME) %>%
  mutate(HHINCOME = as.numeric(HHINCOME), AGE = as.numeric(AGE)) %>%
  filter(!is.na(HHINCOME), !is.na(AGE)) %>%
  filter(between(AGE, 18, 65)) %>%
  filter(HHINCOME > 0)
# check and see the data we are implementing for the model that we are actually incorperating after the generic age filter

dim(data)  #if you want to view the two filtered columns

IQR_of_AGE <- IQR(data$AGE, na.rm = TRUE)
IQR_of_HHINCOME <- IQR(data$HHINCOME, na.rm = TRUE)

# calculating the upper and lower bounds of both of the data sets to filter the data
AGE_lower <- quantile(data$AGE, 0.25, na.rm = TRUE) - 1.5 * IQR_of_AGE
AGE_upper <- quantile(data$AGE, 0.75, na.rm = TRUE) + 1.5 * IQR_of_AGE

HHINCOME_lower <- quantile(data$HHINCOME, 0.25, na.rm = TRUE) - 1.5 * IQR_of_HHINCOME
HHINCOME_upper <- quantile(data$HHINCOME, 0.75, na.rm = TRUE) + 1.5 * IQR_of_HHINCOME

#continue to filter any of the outliers that are presents in the data set
filtered_data <- data %>%
  filter(AGE >= AGE_lower & AGE <= AGE_upper)%>%
    filter(HHINCOME >= HHINCOME_lower & HHINCOME <= HHINCOME_upper)

# we check and see the data sets we have filtered out of here
dim(filtered_data) 

```

## Original Histogram

### Age

We can see in the histogram for Age that there is a relatively normal distribution with no extreme skew or any outliers present at all.

```{r}
ggplot(data, aes(x = AGE)) +
  geom_histogram(binwidth = 5, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Age", x = "Age", y = "Frequency") +
  theme_minimal()
```

### Household Income

We can see here that pre-filtering our data has a very large right skew and an extremely large outlier present in the data set.This is due to the fact that most individuals earn a relatively modest and moderate incomes.

It's likely this data set interviewed a lot of people who come from this category

```{r}
ggplot(data, aes(x = HHINCOME)) +
  geom_histogram(binwidth = 10000, fill = "green", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Household Income", x = "Household Income", y = "Frequency") +
  theme_minimal()
```

## Filtered Histogram

### AGE

We can see in the histogram for Age that there is a relatively normal distribution with no skew and major outliers.
so we have a fairly balanced data set.

```{r}
ggplot(filtered_data, aes(x = AGE)) +
  geom_histogram(binwidth = 5, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Age", x = "Age", y = "Frequency") +
  theme_minimal()
```

### Household Income

Although the data set still contains a right skew, The data is a lot better of a fit for this instance.
There are no extreme outliers and actual as mentioned before that skew is bound to be prevalent over the individual as most house holds in the data set earn a relatively modest income

```{r}
ggplot(filtered_data, aes(x = HHINCOME)) +
  geom_histogram(binwidth = 10000, fill = "green", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Household Income", x = "Household Income", y = "Frequency") +
  theme_minimal()
```

## Box Plots of original data

### AGE
```{R}
boxplot(data$AGE, main = "Boxplot of Age", col = "lightblue", ylab = "Age")
```

### Household Income
```{R}
boxplot(data$HHINCOME, main = "Boxplot of Household Income", col = "lightgreen", ylab = "Household Income", ylim = c(0, 1000000))
```


# Analysis
```{r}
# modifying data into a training set and a testing set
set.seed(1)
# ask about a good metric for the split of the data
split <- sample.split(filtered_data$HHINCOME, SplitRatio = 0.98)
train_set <- subset(filtered_data, split == TRUE)
test_set <- subset(filtered_data, split == FALSE)

#sized of the sets
cat(paste("Size of the training set (rows):", nrow(train_set), "\n"))
cat(paste("Size of the testing set (rows):", nrow(test_set), "\n"))

#model from the training data
linear_model <- lm(HHINCOME ~ AGE, data = train_set)

# Prediceted values on the test set
test_set$predicted_HHI <- predict(linear_model, newdata = test_set)

# calculate residuals for the test set
test_set$residuals <- test_set$HHINCOME - test_set$predicted_HHI


# Display equation information for the simple linear regression
linear_model
```

For the data set, we implemented 98% of filtered data options goes towards training our model. The large percentage was used due to the Extreme scale of data we are implementing. The Testing set implemented 2% of the data and we implemented for the testing and assessment of our regression algorithm. 

Thus Through our training data we get the The follwing regression model
$$
HHINCOME = 115580.53 + 33.32 \times AGE
$$
$$ 
y = 115580.53 + 33.32 x_1
$$

## Implementing the Plots

### Linear Regression with Training Data
```{r}
plot(train_set$AGE, train_set$HHINCOME, main="HHINCOME vs AGE", xlab="Age", ylab="Household Income")

# Fit the linear regression model
model <- lm(HHINCOME ~ AGE, data = train_set)

# Add the regression line to the plot
abline(model, col="red")

```
We can see with the diaganol with the original training set is that it is almost impossible to deter how good of a fit the line is. There are simply to many points in the model to decipher a strong linear regression.

### Linear Regression with Testing Data
This plot is also referenced in the next session for further assesment.
```{r}

plot(test_set$AGE, test_set$HHINCOME, main="HHINCOME vs AGE", xlab="Age", ylab="Household Income")

# Fit the linear regression model
model <- lm(HHINCOME ~ AGE, data = train_set)

# Add the regression line to the plot
abline(model, col="red")

```

From our model we can see that the linear regression line is an extremely poor fit and in fact actually resembles the set up of the residual vs fitted value graphs.
This output further suggest that a linear regression model is not appropriate for the data, as the residuals show the lower cluster patterns, and the seperate data points for the testing points appear to fail to encompass the model with any sort of trends.

### Summary of the Simple Linear Regression Model

```{r}
summary(linear_model)
```

From the Residual ranges we can see that the linear model has some very large error when it comes to underestimating and overestimating a US citizens house hold income.
Our coefficient for the Age of a person is 33.32 meaning that the model predicts that for every additional year of age, household income increases by 33.32 dollars on average.
this is an extremely small change which can only mean the age alone is not a very strong depiction of income in our linear model.


With this Our predicted linear equation is 

# Model Evaluation and Prediction

### Model Assessment

**Residual Analysis:** - **Residual vs. Fitted Values Plot:** The plot shows how residuals are distributed.
Ideally, they should be randomly scattered around the horizontal axis.**Normal Q-Q Plot:** The Q-Q plot assesses if the residuals follow a normal distribution.
Any systematic departures from the diagonal line suggest non-normality.

```{r}
ggplot(test_set, aes(x = predicted_HHI, y = residuals)) +
  geom_point(alpha = 0.5, color = 'black') +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals vs. Fitted Values",
       x = "Fitted Values",
       y = "Residuals") +
  theme_minimal()
```

```{r}
ggplot(test_set, aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line() +
  labs(title = "Normal Q-Q Plot of Residuals") +
  theme_minimal()
```

**Goodness-of-Fit:** - **R-squared Value:** Indicates the proportion of variance in the dependent variable explained by the model.
- **Adjusted R-squared Value:** Adjusted for the number of predictors in the model.

```{r}
adjusted_r_squared <- summary(linear_model)$r.squared
adjusted_r_squared

```

```{r}
adjusted_r_squared <- summary(linear_model)$adj.r.squared
adjusted_r_squared

```

### Model Accuracy

**Error Metrics:** - **Mean Absolute Error (MAE):** Measures the average magnitude of errors without considering their direction.
- **Mean Squared Error (MSE):** Measures the average squared difference between actual and predicted values.
- **Root Mean Squared Error (RMSE):** Provides error in the same units as the response variable.
- **Mean Absolute Percentage Error (MAPE):** Indicates accuracy as a percentage.

```{r}
r <- as.vector(test_set$residuals)
mse <- mean(r^2)
mse
# r <- as.numeric(test_set$residuals)
# mse <- mean((r)^2)
# mse

```

### Prediction

**Prediction vs. Actual Plot:**

```{r}
#
model <- lm(HHINCOME ~ AGE, data = train_set)

# Predict HHINCOME on the test set using the model
predictions <- predict(model, newdata = test_set)

# Plot: Actual HHINCOME vs. Predicted HHINCOME
plot(test_set$AGE, test_set$HHINCOME, main="Prediction vs Actual", 
     xlab="Age", ylab="Household Income", pch=19, col="blue") # Actual data points
points(test_set$AGE, predictions, col="red", pch=19)


```

# Conclusion and Summary

# Reference
